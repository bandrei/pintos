            +-------------------+
            |       OS 211      |
            |  TASK 1: THREADS  |
            |  DESIGN DOCUMENT  |
            +-------------------+
                   
---- GROUP ----

Rory Allford <rda10@imperial.ac.uk>
Andrei Bara <ab6610@imperial.ac.uk>
Alina Boghiu <ab3110@imperial.ac.uk>

---- PRELIMINARIES ----
>> Note that when calculating the maximum or minimum
>> from a list we have not used the already existing
>> function (i.e. list_get_max and functions for defining
>> the order of elements) as we considered it to be more 
>> resource efficient although the amount of code is increased
>> (function calls and the structures created within each function
>> call could be quite high) the point was for most of the code
>> to execute as fast as possible wher interrupts are disabled
>> (so is the case when waking threads contained in
>> semaphores, and the waiting_list).


                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

~~~~~~~~~~~~thread.c~~~~~~~~~~~~~~~~~~~~~~~~~~~

*******static struct list waiting_list;*********
PURPOSE:
>> This struct will hold a list of sleeper elements
>> used to store information about the threads that
>> are put to sleep. 

*******static struct lock waiting_lock;*********
PURPOSE:
>> Create a lock to be used to avoid race conditions
>> when trying to insert a new element into the waiting_list
>> (It can't be used in an interrupt handler as it is a lock)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


~~~~~~~~~~~~~~~~~~~thread.h~~~~~~~~~~~~~~~~~~~~~
void thread_sleep(int64_t ticks);
	- will put a thread asleep untill "ticks"
void thread_wake(int64_t ticks);
	- will wake up al the threads if "ticks" ticks
	have passed
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


~~~~~~~~~~~~~~~~~~synch.h~~~~~~~~~~~~~~~~~~~~~~
struct sleeper
  {
  int64_t wake_time;
  struct semaphore waiting_semaphore;
  struct list_elem elem;
  }; 

PURPOSE:
>> Will store information about the threads that are being put asleep.
>> It will contain information about the sleeping time (which is actually
>> the time when a thread should be woken up) and a semaphore that will be
>> downed by the sleeping thread.
>> To be used in conjuction with "waiting_list".

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

The timer_sleep(int64_t ticks) method calls thread_sleep(int64_t ticks) which puts the
current thread to sleep untill (start+ticks) number of timer ticks, where start represents
the number of ticks so far. It then calls thread_sleep() with an argument representing 
the time when the thread is due to wake up. thread_sleep() will then add the current 
running thread to the waiting_list by creating a semaphore, adding it to the list and then 
downing it (i.e. the sleeper struct is used in order to accomplish this). 
The current thread is then blocked and schedule() will be called.

Also the timer interrupt handler will be the one incrementing the number of system tick
and the one that will call thread_tick() which in turn will call thread_wake(). At this
point the list of "sleepers" will be traversed in ascending order and the appropriate threads
woken up.



>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

In order to minimize the time spent waking the threads up the threads that
have been put to sleep have been inserted in ascending order of their wake up time (stored
as an absolute value from when the operating system has started, in the wake_time member).
Thus, when waking up threads the list of sleepers will be popped untill all the threads
with the same wake_time value have been woken up (and then break early as the list is ordered).

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

In order to avoid race condition when timer_sleep() is called and thus
thread_sleep(), we have used locks and semaphores. When inserting a new
sleeper struct in the waiting_list we acquire waiting_lock, perform the 
insertion and then release the lock. After this point the thread is being put to sleep.
(Note: when the thread is woken up it will continue running from where it was stopped
in thread_sleep(); it will remove itself from the list of waiters).

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

First scenario would involve changing the state of the waiting_list inside the
thread_sleep function by being interrupted while inserting a sleeper element in the list.
No special measures are needed to avoid this scenario as the current definition of list_insert
would not leave the waiting_list in an inconsistent state at any time (this is due to the fact
that the operations when the pointers themselves are changed, are atomic; possibly a "mov" instruc-
tion).

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We have chosen this design as this seemed one of the best ways to do it:
1. We are using synchronization mechanism already implemented in PintOS (like locks
and semaphores) as opposed to using thread_block() and thread_unblock() to emulate
the behaviour of the already existing structures. Also as time is not an issue when
calling thread_sleep() there is no immediate requirement for code optimisations.
2. We are avoiding any possible race conditions by using synchronization
3. The code is easier to understand by using locks and semaphores
4. Keeping absolute values of sleeping times reduces computational costs and also
simplifies the algorithms as opposed to relative sleeping times which would allow the user
to set a longer sleeping time but would slow down the operating system (as the computations
for deciding which thread to be woken up is done during an interrupt).
5. Using for loops inside thread_wake instead of making use of list_max or list_min
will optimize performance as this function is being called every tick and thus reducing
the complexity of function calls and reducing the stack usage is required.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

~~~~~~~~~~~~~~~~~~~~~~~~thread.h~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct thread
  {
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */
    int priority;                       /* Priority. */
    int init_priority;			/* Initial priority when donation starts
					   (i.e. priority to revert to when all
					   of the donations have been removed */
    struct lock *try_lock;		/* Hold the lock the current thread is
					   trying to lock on */
    struct list lock_list;		/* Hold a list of locks that another
					   thread is trying to acquire from the
					   current thread */
    struct list_elem allelem;		/* List element for all threads list. */

    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */
        
    /* BSD */
    fixed recent_cpu;
    int nice;

#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif

    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */
  };
----------------------------------------------------------------------------
New memebers in struct threads:
----------------------------------------------------------------------------
int init_priority;
    - Holds the initial priority to which the thread reverts to after donation.

struct lock *try_lock;
    - Points to the lock the current thread is trying to lock on.

struct list lock_list;
    - Holds a list of locks that other threads are trying to acquires from the
       current thread.

fixed recent_cpu; --for later use in the BSD
int nice;		  --for later use in the BSD
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~





~~~~~~~~~~~~~~~~~~~~~~~~~~synch.h~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct lock 
  {
    struct thread *holder;      /* Thread holding lock (for debugging). */
    struct semaphore semaphore; /* Binary semaphore controlling access. */
	int priority;
	struct list_elem elem;		/* will e */
  };

-----------------------------------------------------------------------------
New memebers:
-----------------------------------------------------------------------------
int priority 
	- will store the priority of the thread that it is trying to acquire 
	that lock if its priority is higher than the already existing priority
	of the lock

struct list_elem elem
	- used for inserting the lock in a list

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~synch.h~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct semaphore 
  {
    volatile unsigned value;             /* Current value. */
    struct list waiters;        /* List of waiting threads. */
  };
  
  value has been declared as volatile in order to avoid any problems that 
  might occur due to context switches and compiler optimisations (this
  was not a necessary change, but it better to avoid possible error that
  might occur; the optimisation barrier should reduce the proabability of such
  an error occuring). 
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~







~~~~~~~~~~~~~~~~~~~~~~~~~~~~~thread.c~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
static struct list priority_list [PRI_MAX+1];  

The variable will hold 64 priority lists that will be used by the threads,
scheduling and the rest of the operating system. All of the THREAD_READY
threads should be in one of these lists

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

The "data structure" used to track priority donation is formed of 2 components:
1. lock_list - as explained above
2. struct lock - by making use of the newly added priority field.



------------------------------------------------------------|
                                                            |
THREAD 1                                                    |
trying LOCK A                                               |
priority: 20;                                               |
------------------------------------------------------------|
                        |
                        |
                        |
------------------------|---------------------------------------|
                       \\/                                      |
                    -------------	-------------               |
THREAD 1            |  LOCK     |   |   LOCK     |              |
trying LOCK B       |   A       |	|     C      |              |
                    |pri: 20    |	|            |              |
priority:17;        -------------	--------------              |
new priority: 20;	locks held by the current thread            |
                    that another thread is trying to acquire    |
                    (locks stored in lock_list)                 |
----------------------------------------------------------------|
                        |
                        |
                        |
------------------------|---------------------------------------|
                       \\/                                      |
                        -------------	-------------           |
THREAD 2                |   LOCK    |   |   LOCK    |           |
priority: 16;           |     B	    |	|     D	    |           |
new priority: 20        |    pri: 20|	|           |           |
                        -------------	-------------           |
                      locks held by the current thread          |
                      that another thread is trying to acquire  |
                        (locks stored in lock_list)             |
----------------------------------------------------------------|

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

For the case of locks and semaphores (as locks are using semaphores
in their construction) when a thread has to be woken up by a call to
sema_up() the list of waiters is traversed and the thread with the
maximum priority is unblocked.

In the case of condition variables where multiple semaphores are used
(i.e. a semaphore for each thread that calls cond_wait()) the semaphore
containing the maximum thread is upped (in this case the default list_max
function is used in conjuction with cond_var_less to return the maximum
semaphore as extreme time efficiency is not required here).


In all of the cases sema_up will call thread_yield() as soon as possible
if nod inside an interrupt (if in an external interrupt handler then 
intr_yield_on_return() is called so that the thread yields when it returns from
the interrupt handler) or if the interrupts are not disabled. This will allow
the thread with the highest priority to be scheduled immediately if possible.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

Firstly, lock_acquire will disble interrupt prior to the start of the donation
process and then check if the lock that the current thread is trying
to acquire is indeed held by another thread. If such is the case then the current thread's
try_lock member will point to this lock and the donation process is initiated by calling
lock_donate.

Lock donate will handle nested donation up to 8 nested threads as long as
the thread that is being donated to is different from the current thread 
(both conditions will prevent and will speed up the donation process ). 
Also the priority donation process only takes place if the thread that is 
donating has higher pirority than the thread it's donating to.
Then the lock that is trying to be acquired records the priority of the donating thread
and gets added to the lock_list of its holding thread. Then, if the holding thread is 
THREAD_READY then thread_swap() is called and this function will move the holding thread 
in the lists of ready threads otherwise its priroity gets changed but the thread is not moved
to a new list/location.  If the holding thread is also trying to acquire another lock 
(by checking try_lock) the process continues up to 8 times, otherwise
the nested donation stops.

It is important to note that if it is the case that the lock_list is empty then
the init_priority field in the holding thread (i.e. the one that will receive the donation) 
will be set to its priority prior to donation (this step is required when releasing the donations
and there is nothing more to cascade through). Also a lock is only added once to the list,
and if already in the list then its priority gets updated.

Interrupts are renabled after the donation has completed.


>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.


When a lock is released it is removed from the lock_list of the holding thread.
If the list is empty then lock_donate_restore will return the initial priority of
the thread. Otherwise it will return the maximum priority recorded by the locks
in lock_list (see the donation process above).
Then the priority of the current thread is set to the new priority before calling sema_up.

Like in the donation process the donation restoration process will disable and then re-enable
interrupts.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

A potential race condition in thread_set_priority might arrise when a donation process
is taking place. Although only the current thread can make use of thread_set_priority()
during a donation the thread's priority could be changed and thus leaving the priority
switching operations in a incosistent state and leading to undefined behaviour). 

Therefore in order to avoid this and any other potential race conditions we have decided
to disable interrupt for a very short period of time while manipulating the priority of
the thread.

Using a lock at this point might be inappropriate as for instance the thread could yield
after acquiring the lock, but then another thread might at some point want to donate to
this thread and since this thread has already acquired a lock to prevent priority change
the donation process as the priority of the thread can no longer be modified thus leading
to a potential deadlock situation.


---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We have chosen this design due to the constraits of efficiency and also
for reasons of simplicity as explained below:

1. In the case of semaphores we have avoided using ordered insertion and
other already existing list function. Due to priority donation and the priorities
changing at any time keeping the waiters list of a semaphore ordered would require
a lot of computational resources and thus slow the program significantly and even
miss on some interrupts. Also calls like list_max are to avoided due to the fact
that each function call adds to the complexity and also the operating system
would have to jump to different memory locations to execute that function call
whereas with our current implementation the code is trully inline. 
	For the case of conditional variables (and sleeping threads as above) such
constraints are not required as the signaling and waiting call are or should
be used in a non-interrupted context and thus the small improvement in efficiency
can be traded-off for a more clear code.

2. We have modified the struct of the lock to hold a priority field and an elem field.
This is due to the fact that it is easier during the donation restoration process
to remove the locks from the lock_list of the holder thread and also dinamically change
their recorded priority. This small trick saves a lot of both memory space and algorithm
complexity.

3. In the thread structure we have added 2 new fields. try_lock is used to allow nested
donation and it is an easy way to record which lock a thread is trying to acquire and since
threads are visible within the scope of the locks (i.e. by using lock.holder). init_priority
records the priority of a thread before receiving its first donation. By adding these 2 fields
a lot of complexity has been avoided by not creating separate data structures that would just
store this information.

4. We have used an array of size 64 to store 64 priority lists. This is due to the fact
that this provides 64 adjacent memory location, that are usually faster to access when
operating on this array and on the lists (i.e. for instance when next_thread_to_run() 
iterates through the array to find the next non-empty list of threads). Also the lists provide
functionalities for adding at the front or back at the list in constant time; same happens
when popping the list. Thus the complexity of moving a thread in the ready lists is constant.
Although keeping an array of 64 lists in memory takes 1KB of space, this is still a vital
part of the operating system which should allow very fast access to its elements. For future
optimisation we might look at a new type of data structure that would allow better performace.


              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

				IN THREAD.C
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct thread
  {
    fixed recent_cpu;
    int nice;
  };

#define NICE_MIN -20
#define NICE_MAX 20

static volatile fixed load_avg;
static volatile fixed ready_threads;

fixed la_past_weight;
fixed la_cur_weight;
fixed fp_pri_max;

void thread_tick_mlfqs (int64_t ticks);
void thread_set_priority_mlfqs (int new_priority);

----------------------------------------------------------------------
New members:
----------------------------------------------------------------------
 thread.recent_cpu
	- fixed point recent_cpu per-thread statistic calculated as specified
 thread.nice
	- integer per-thread nice value
	- Invariant: clamped between NICE_MIN and NICE_MAX
 load_avg
	- fixed point global load average statistic
 ready_threads
	- integer count of the number of threads with status READY or RUNNING
	- Invariant: value is updated consistently with thread.priority
	- READY_THREADS_CHECK can be set to assert this every tick
 la_past_weight, la_cur_weight, fp_pri_max
	- fixed point constants to optimise certain calculations every tick
 thread_tick_mlfqs, thread_set_priority_mlfqs
	- mlfqs entrypoints for these functions that are set on startup
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

							IN THREAD.H

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
typedef void fp_thread_tick (int64_t ticks);
fp_thread_tick *thread_tick;

typedef void fp_thread_set_priority (int new_priority);
fp_thread_set_priority *thread_set_priority;

----------------------------------------------------------------------
Function pointers used to make switching entry points between the 
mlfqs and normal kernel scheduling modes clearer and more efficient
- we save an if(thread_mlfqs) statement every tick.
---------------------------------------------------------------------
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0		0	0	0	63	61	59	  A
 4		4 	0	0	62	61	59	  A
 8		8	0	0	61	61	59	  B
12		8	4	0	61	60	59	  A
16		12 	4	0	60 	60 	59	  B
20		12	8	0	60 	59 	59 	  A
24		16	8 	0 	59	59 	59 	  C
28		16	8	4	59	59	58	  B
32		16	12	4	59	58	58 	  A
36		20	12	4	58	58	58 	  C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behaviour of your scheduler?

One ambiguity in the specification is whether to increment the recent_cpu
value before re-calculating the priorities or after calculating the priority.
In our implementation as reflected in the table we increment it before so that
the value used in the other calculations is as accurate and recent as possible.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

At all stages we have attempted to minimize the number of operations inside the
timer interrupt. For instance rather than re-counting ready_threads we maintain
this as a global variable outside the interrupt context which also gives an
expected amortised speedup. We have also used the function pointer entrypoints
detailed above to avoid if(thread_mlfqs) checks.

There is a negligable O(1) addition to the critical section in thread creation.

Reducing the size of critical sections in particular the complexity of 
thread_tick leads to an amortised speedup in the scheduling performance.

Since Pintos is not a multiprocessing OS further changes that could increase
performance such as pre-emptible scheduling code or a finer grained locking
approach would not yield any improvement in this case.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the task, how might you choose to
>> refine or improve your design?

In many of the situations we have decided to implement macros rather
than functions. Although this is not usually good practice this would
reduce the overhead caused by function calls.

Also in order to speed up the calculation we have used bitwise arithmetic
when multiplying or dividing by multiples of 2 (although in most of the cases 
the compiler would optimise this, due to the optimisation barrier imposed
on pintos this would probably not happen).
 
It is important to note that some of the function have been declared as inline
(like thread_calc_recent_cpu or thread_calc_priority_mlfqs). This has been
done to speed up the calculations processes in interrupt by avoiding function
calls in loop and rather inserting the code of the function inisde the loop.

Some of the the variables like ready_threads have been declared as volatile
in order to avoid any possible conflicts that might occur when the compiler
tries to optimize the code and does register allocation (although during testing
we have not encountered a scenario in which this might occur this is still
a good thing to do).

For future optimisation we might have to look at ways of computing the values
in as few steps as possible and also see if some more of the computations can
be moved out of the interrupt. Also we might research a new type of storing
the ready threads which would allow faster traversals and searches.

>> C6: The assignment explains arithmetic for fixed-point mathematics in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point mathematics, that is, an abstract
>> data type and/or a set of functions or macros to manipulate
>> fixed-point numbers, why did you do so?  If not, why not?

For the implementation of fixed-point arithmetic we have decided
to use macros in order to compute the values required. As most of the
fixed-point arithmetic is done inside the timer-interrupt speed is an
issue and reducing the overhead of function calls is important.

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining tasks?

>> Any other comments?
