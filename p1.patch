diff --git src/Makefile.build src/Makefile.build
index e997d27..49e98e8 100644
--- src/Makefile.build
+++ src/Makefile.build
@@ -21,6 +21,7 @@ threads_SRC += threads/intr-stubs.S	# Interrupt stubs.
 threads_SRC += threads/synch.c		# Synchronization.
 threads_SRC += threads/palloc.c		# Page allocator.
 threads_SRC += threads/malloc.c		# Subpage allocator.
+threads_SRC += threads/fixedpoint.c	# Fixed point arithmetic
 
 # Device driver code.
 devices_SRC  = devices/pit.c		# Programmable interrupt timer chip.
diff --git src/devices/timer.c src/devices/timer.c
index befaaae..a070963 100644
--- src/devices/timer.c
+++ src/devices/timer.c
@@ -92,8 +92,7 @@ timer_sleep (int64_t ticks)
   int64_t start = timer_ticks ();
 
   ASSERT (intr_get_level () == INTR_ON);
-  while (timer_elapsed (start) < ticks) 
-    thread_yield ();
+  thread_sleep(start+ticks);
 }
 
 /* Sleeps for approximately MS milliseconds.  Interrupts must be
@@ -171,7 +170,7 @@ static void
 timer_interrupt (struct intr_frame *args UNUSED)
 {
   ticks++;
-  thread_tick ();
+  thread_tick (ticks);
 }
 
 /* Returns true if LOOPS iterations waits for more than one timer
diff --git src/lib/kernel/list.c src/lib/kernel/list.c
index 316d9ef..204c55c 100644
--- src/lib/kernel/list.c
+++ src/lib/kernel/list.c
@@ -56,6 +56,11 @@ is_tail (struct list_elem *elem)
   return elem != NULL && elem->prev != NULL && elem->next == NULL;
 }
 
+bool is_in_list(struct list_elem *elem)
+{
+	return elem != NULL && elem->prev != NULL && elem->next != NULL;
+}
+
 /* Initializes LIST as an empty list. */
 void
 list_init (struct list *list)
diff --git src/lib/kernel/list.h src/lib/kernel/list.h
index f1f12e9..5e15a52 100644
--- src/lib/kernel/list.h
+++ src/lib/kernel/list.h
@@ -135,6 +135,13 @@ struct list_elem *list_rend (struct list *);
 
 struct list_elem *list_head (struct list *);
 struct list_elem *list_tail (struct list *);
+//extra function used to check if an element is 
+//in any list. In order for it to work properly
+//when an element is removed from the list its
+//prev and next pointers need to be set to NULL
+//(note: this function is used only in conjuction
+//with the code for lock_list in synch.c)
+bool is_in_list(struct list_elem *);
 
 /* List insertion. */
 void list_insert (struct list_elem *, struct list_elem *);
diff --git src/threads/DESIGNDOC src/threads/DESIGNDOC
new file mode 100644
index 0000000..d8e0fa4
--- /dev/null
+++ src/threads/DESIGNDOC
@@ -0,0 +1,162 @@
+            +-------------------+
+            |       OS 211      |
+            |  TASK 1: THREADS  |
+            |  DESIGN DOCUMENT  |
+            +-------------------+
+                   
+---- GROUP ----
+
+>> Fill in the names and email addresses of your group members.
+
+Rory Allford <rda10@imperial.ac.uk>
+FirstName LastName <email@imperial.ac.uk>
+FirstName LastName <email@imperial.ac.uk>
+
+---- PRELIMINARIES ----
+
+>> If you have any preliminary comments on your submission, notes for the
+>> TAs, or extra credit, please give them here.
+
+>> Please cite any offline or online sources you consulted while
+>> preparing your submission, other than the Pintos documentation, course
+>> text, lecture notes, and course staff.
+
+                 ALARM CLOCK
+                 ===========
+
+---- DATA STRUCTURES ----
+
+>> A1: Copy here the declaration of each new or changed `struct' or
+>> `struct' member, global or static variable, `typedef', or
+>> enumeration.  Identify the purpose of each in 25 words or less.
+
+---- ALGORITHMS ----
+
+>> A2: Briefly describe what happens in a call to timer_sleep(),
+>> including the effects of the timer interrupt handler.
+
+>> A3: What steps are taken to minimize the amount of time spent in
+>> the timer interrupt handler?
+
+---- SYNCHRONIZATION ----
+
+>> A4: How are race conditions avoided when multiple threads call
+>> timer_sleep() simultaneously?
+
+>> A5: How are race conditions avoided when a timer interrupt occurs
+>> during a call to timer_sleep()?
+
+---- RATIONALE ----
+
+>> A6: Why did you choose this design?  In what ways is it superior to
+>> another design you considered?
+
+             PRIORITY SCHEDULING
+             ===================
+
+---- DATA STRUCTURES ----
+
+>> B1: Copy here the declaration of each new or changed `struct' or
+>> `struct' member, global or static variable, `typedef', or
+>> enumeration.  Identify the purpose of each in 25 words or less.
+
+>> B2: Explain the data structure used to track priority donation.
+>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
+>> .png file.)
+
+---- ALGORITHMS ----
+
+>> B3: How do you ensure that the highest priority thread waiting for
+>> a lock, semaphore, or condition variable wakes up first?
+
+>> B4: Describe the sequence of events when a call to lock_acquire()
+>> causes a priority donation.  How is nested donation handled?
+
+>> B5: Describe the sequence of events when lock_release() is called
+>> on a lock that a higher-priority thread is waiting for.
+
+---- SYNCHRONIZATION ----
+
+>> B6: Describe a potential race in thread_set_priority() and explain
+>> how your implementation avoids it.  Can you use a lock to avoid
+>> this race?
+
+---- RATIONALE ----
+
+>> B7: Why did you choose this design?  In what ways is it superior to
+>> another design you considered?
+
+              ADVANCED SCHEDULER
+              ==================
+
+---- DATA STRUCTURES ----
+
+>> C1: Copy here the declaration of each new or changed `struct' or
+>> `struct' member, global or static variable, `typedef', or
+>> enumeration.  Identify the purpose of each in 25 words or less.
+
+---- ALGORITHMS ----
+
+>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
+>> has a recent_cpu value of 0.  Fill in the table below showing the
+>> scheduling decision and the priority and recent_cpu values for each
+>> thread after each given number of timer ticks:
+
+timer  recent_cpu    priority   thread
+ticks   A   B   C   A   B   C   to run
+-----  --  --  --  --  --  --   ------
+ 0
+ 4
+ 8
+12
+16
+20
+24
+28
+32
+36
+
+>> C3: Did any ambiguities in the scheduler specification make values
+>> in the table uncertain?  If so, what rule did you use to resolve
+>> them?  Does this match the behaviour of your scheduler?
+
+>> C4: How is the way you divided the cost of scheduling between code
+>> inside and outside interrupt context likely to affect performance?
+
+---- RATIONALE ----
+
+>> C5: Briefly critique your design, pointing out advantages and
+>> disadvantages in your design choices.  If you were to have extra
+>> time to work on this part of the task, how might you choose to
+>> refine or improve your design?
+
+>> C6: The assignment explains arithmetic for fixed-point mathematics in
+>> detail, but it leaves it open to you to implement it.  Why did you
+>> decide to implement it the way you did?  If you created an
+>> abstraction layer for fixed-point mathematics, that is, an abstract
+>> data type and/or a set of functions or macros to manipulate
+>> fixed-point numbers, why did you do so?  If not, why not?
+
+               SURVEY QUESTIONS
+               ================
+
+Answering these questions is optional, but it will help us improve the
+course in future quarters.  Feel free to tell us anything you
+want--these questions are just to spur your thoughts.  You may also
+choose to respond anonymously in the course evaluations at the end of
+the quarter.
+
+>> In your opinion, was this assignment, or any one of the three problems
+>> in it, too easy or too hard?  Did it take too long or too little time?
+
+>> Did you find that working on a particular part of the assignment gave
+>> you greater insight into some aspect of OS design?
+
+>> Is there some particular fact or hint we should give students in
+>> future quarters to help them solve the problems?  Conversely, did you
+>> find any of our guidance to be misleading?
+
+>> Do you have any suggestions for the TAs to more effectively assist
+>> students, either for future quarters or the remaining tasks?
+
+>> Any other comments?
diff --git src/threads/fixedpoint.c src/threads/fixedpoint.c
new file mode 100644
index 0000000..984b137
--- /dev/null
+++ src/threads/fixedpoint.c
@@ -0,0 +1,54 @@
+#include "fixedpoint.h"
+
+inline fp_int fp_clampi(fp_int n,fp_int l,fp_int h) {
+  return n<l ? l : ( n>h ? h : n );
+}
+
+inline fixed fp_fromint(fp_int n) {
+	return n * FP_F;
+} 
+
+inline fp_int fp_floor(fixed x) {
+	return x / FP_F;
+}
+
+inline fp_int fp_round(fixed x) {
+	return (x<0) ? (x - FP_F/2)/FP_F : (x + FP_F/2)/FP_F;
+}
+
+inline fixed fp_inc(fixed x) {
+    return x + FP_F;
+}
+
+inline fixed fp_add(fixed x, fixed y) {
+      return x + y;
+}
+ 
+ 
+inline fixed fp_sub(fixed x, fixed y) {
+      return x - y;
+}
+
+inline fixed fp_mul(fixed x, fixed y) {
+	return ((fixed_large)x) * y / FP_F;
+}
+
+inline fixed fp_div(fixed x, fixed y) {
+	return ((fixed_large)x) * FP_F / y;
+}
+
+inline fixed fp_addi(fixed x, fp_int n) {
+	return x + n * FP_F;
+}
+
+inline fixed fp_subi(fixed x, fp_int n) {
+	return x - n * FP_F;
+}
+
+inline fixed fp_muli(fixed x, fp_int n) {
+	return x * n;
+}
+
+inline fixed fp_divi(fixed x, fp_int n) {
+	return x / n;
+}
\ No newline at end of file
diff --git src/threads/fixedpoint.h src/threads/fixedpoint.h
new file mode 100644
index 0000000..76b1f0f
--- /dev/null
+++ src/threads/fixedpoint.h
@@ -0,0 +1,102 @@
+#ifndef THREADS_FIXEDPOINT_H
+#define THREADS_FIXEDPOINT_H
+
+typedef int32_t fixed;
+typedef int32_t fp_int; 
+typedef int64_t fixed_large;
+// 31 bit signed integer + 1 sign bit
+#define FP_COUNT 14
+// sign bit, 17bits . 14 bits
+
+static fp_int const _fp_f = 0x1<<FP_COUNT;
+#define FP_F _fp_f
+// FP_F = 2^FP_COUNT
+
+inline fp_int fp_clampi(fp_int n,fp_int l,fp_int h);
+inline fixed fp_fromint(fp_int n);
+inline fp_int fp_floor(fixed x);
+inline fp_int fp_round(fixed x);
+inline fixed fp_inc(fixed x);
+inline fixed fp_add(fixed x, fixed y);
+inline fixed fp_sub(fixed x, fixed y);
+inline fixed fp_mul(fixed x, fixed y);
+inline fixed fp_div(fixed x, fixed y);
+inline fixed fp_addi(fixed x, fp_int n);
+inline fixed fp_subi(fixed x, fp_int n);
+inline fixed fp_muli(fixed x, fp_int n);
+inline fixed fp_divi(fixed x, fp_int n);
+
+/**
+ * FP_CLAMPI(N,L,H)
+ * clamp fp_int N in range [L,H]
+ * return fp_int
+ **/
+
+#define FP_CLAMPI(N,L,H) (N)<(L) ? (L) : ( (N)>(H) ? (H) : (N) )
+
+/**
+ * FP_FROMINT(N)
+ * convert fp_int N to fixed
+ * return fixed
+ **/
+ 
+
+
+#define FP_FROMINT(N) (N) * FP_F
+
+
+/**
+ * FP_FLOOR(X)
+ * round fixed X to 0
+ * return fp_int
+ **/
+ 
+
+#define FP_FLOOR(X) (X) / FP_F
+
+/**
+ * FP_ROUND(X)
+ * round fixed X to nearest
+ * return fp_int
+ **/
+
+ 
+#define FP_ROUND(X) ((X) < 0) ? ((X) - (FP_F>>2))/FP_F : ((X) + (FP_F>>2))/FP_F
+
+
+
+#define FP_INC(X) (X) + FP_F
+
+/**
+ * FP_OP(X,Y)
+ * fixed X OP fixed Y
+ * return fixed
+ **/
+
+#define FP_ADD(X,Y) (X) + (Y)
+
+
+#define FP_SUB(X,Y) (X) - (Y)
+
+
+#define FP_MUL(X,Y) (fixed) ( ((fixed_large) (X)) * (Y) / FP_F )
+
+#define FP_DIV(X,Y) (fixed) ( ((fixed_large) (X)) * FP_F / (Y) )
+
+/**
+ * FP_OP(X,N)
+ * fixed x OP fp_int n
+ * return fixed
+**/
+
+#define FP_ADDI(X,N) (X) + (N) * FP_F
+
+
+#define FP_SUBI(X,N) (X) - (N) * FP_F
+
+
+#define FP_MULI(X,N) (X) * (N)
+
+#define FP_DIVI(X,N) (X) / (N)
+
+#endif /* threads/fixedpoint.h */
\ No newline at end of file
diff --git src/threads/synch.c src/threads/synch.c
index 317c68a..5efd39c 100644
--- src/threads/synch.c
+++ src/threads/synch.c
@@ -31,6 +31,7 @@
 #include <string.h>
 #include "threads/interrupt.h"
 #include "threads/thread.h"
+#include "threads/malloc.h"
 
 /* Initializes semaphore SEMA to VALUE.  A semaphore is a
    nonnegative integer along with two atomic operators for
@@ -75,6 +76,7 @@ sema_down (struct semaphore *sema)
   intr_set_level (old_level);
 }
 
+
 /* Down or "P" operation on a semaphore, but only if the
    semaphore is not already 0.  Returns true if the semaphore is
    decremented, false otherwise.
@@ -113,11 +115,32 @@ sema_up (struct semaphore *sema)
   ASSERT (sema != NULL);
 
   old_level = intr_disable ();
-  if (!list_empty (&sema->waiters)) 
-    thread_unblock (list_entry (list_pop_front (&sema->waiters),
-                                struct thread, elem));
+  if (!list_empty (&sema->waiters)) {
+    struct thread *first_awake = NULL;
+	struct thread *tmp_thread;
+	struct list_elem *e;
+    int max_pri = 0;
+	for (e= list_begin (&sema->waiters); e!= list_end (&sema->waiters);
+			 e = list_next (e))
+	{
+		tmp_thread = list_entry(e,struct thread,elem);
+		if(tmp_thread->priority>=max_pri)
+		{
+			max_pri = tmp_thread->priority;
+			first_awake = tmp_thread;
+		}
+    }
+	list_remove(&first_awake->elem);
+	thread_unblock(first_awake);
+	//printf("thread ready to wake");
+    //thread_unblock (list_entry (list_pop_back (&sema->waiters),
+                         //       struct thread, elem));
+  }
   sema->value++;
+ // thread_yield();
   intr_set_level (old_level);
+  if(old_level != INTR_OFF)
+  	thread_yield();
 }
 
 static void sema_test_helper (void *sema_);
@@ -178,6 +201,8 @@ lock_init (struct lock *lock)
   ASSERT (lock != NULL);
 
   lock->holder = NULL;
+  (&lock->elem)->prev = NULL;
+  (&lock->elem)->next = NULL;
   sema_init (&lock->semaphore, 1);
 }
 
@@ -196,9 +221,24 @@ lock_acquire (struct lock *lock)
   ASSERT (!intr_context ());
   ASSERT (!lock_held_by_current_thread (lock));
 
+  	//disable interrupts
+	//if one does not want to wake another 
+ 	//thread that might be trying to acquire 
+ 	//the same lock before actually blocking
+	//the current thread (if it cannot acquire
+	//the lock)
+  	enum intr_level old_level;
+  	old_level = intr_disable();	
+  	if(lock->holder != NULL )
+  	{
+  		thread_current()->try_lock = lock;
+  		lock_donate(lock);
+  	}
+  	intr_set_level(old_level);
   sema_down (&lock->semaphore);
+  thread_current()->try_lock = NULL;
   lock->holder = thread_current ();
-}
+}         
 
 /* Tries to acquires LOCK and returns true if successful or false
    on failure.  The lock must not already be held by the current
@@ -231,7 +271,17 @@ lock_release (struct lock *lock)
   ASSERT (lock != NULL);
   ASSERT (lock_held_by_current_thread (lock));
 
+
+  int new_pri;
+  enum intr_level old_level;
+  old_level = intr_disable();
+  
+  new_pri =lock_donate_restore(lock);
+  
+  intr_set_level(old_level);
+  lock->holder->priority= new_pri;
   lock->holder = NULL;
+	
   sema_up (&lock->semaphore);
 }
 
@@ -245,6 +295,102 @@ lock_held_by_current_thread (const struct lock *lock)
 
   return lock->holder == thread_current ();
 }
+
+/*
+	this might loop up to 8 times
+	Donate priority to the threads holding the lock
+	that the current thread is trying to acquire
+*/
+void lock_donate(struct lock *cur_lock)
+{
+
+	struct lock *next_lock = cur_lock;
+	struct thread *cur_thread = thread_current();
+	int depth = 0; //should donate to up to 8 nested threads
+
+	while(depth<8 && next_lock->holder->priority<cur_thread->priority &&
+		next_lock->holder != cur_thread)
+	{	
+	
+		next_lock->priority = cur_thread->priority;	//memorize the priority in the lock
+													//(can be changed at a later time by 
+													//another donation)
+												
+		//if the list is empty then no donation has taken place before
+		//for the thread that holds the lock. Thus set the init_priority
+		//of the thread that has the lock to its current priority	
+		if(list_empty(&next_lock->holder->lock_list))
+			next_lock->holder->init_priority = next_lock->holder->priority;
+		//if the next_lock is not in the list of locked list then add it
+		if(!is_in_list(&next_lock->elem))
+			list_push_front(&next_lock->holder->lock_list,&next_lock->elem);
+		//it the thread that has the lock is not blocked then move it from
+		//its current priority list to the priority list corresponding to the
+		//priority of the currently running thread
+		if(next_lock->holder->status != THREAD_BLOCKED)
+			thread_swap(next_lock->holder);	
+		else
+			//otherwise just change it's priority as it must be in a waiting
+			//list
+			next_lock->holder->priority = cur_thread->priority;
+			
+		depth++;
+		//check if nested donation can take place
+		if(next_lock->holder->try_lock!=NULL)
+			next_lock = next_lock->holder->try_lock;
+		else
+			break;
+	}
+
+	
+	
+}
+
+/*
+	Restore the donating sequence (i.e. revert the 
+	priorities gained by a thread) and return the priority
+	the current thread should have. 
+*/
+int lock_donate_restore(struct lock *cur_lock)
+{
+	
+	if(is_in_list(&cur_lock->elem)){
+		list_remove(&cur_lock->elem);
+		//remove the current lock from the lock_list
+		//and make sure that the prev and next pointers are set 
+		//to NULL so that is_in_list function will work properly
+		//on the next call on the same element (i.e. see list.c/list.h)
+		(&cur_lock->elem)->prev = (&cur_lock->elem)->next = NULL;
+
+	}
+	
+	struct list_elem *e;
+	struct lock *tmp_lock;
+	int max_pri = 0;
+	//if there are no more donations in the list
+	// return the initial priority
+	if(list_empty(&cur_lock->holder->lock_list))
+		return cur_lock->holder->init_priority;
+	else
+	{
+		//find the max donated priority and return its value
+		for (e= list_begin (&cur_lock->holder->lock_list);
+			 e!= list_end (&cur_lock->holder->lock_list); e = list_next (e))
+		{
+			tmp_lock = list_entry (e, struct lock, elem);
+			if (tmp_lock->priority > max_pri)
+			{
+				max_pri = tmp_lock->priority;
+			
+			}
+   		 }
+		return max_pri;
+	}
+	return 0;
+	
+}
+
+
 
 /* One semaphore in a list. */
 struct semaphore_elem 
@@ -253,6 +399,7 @@ struct semaphore_elem
     struct semaphore semaphore;         /* This semaphore. */
   };
 
+
 /* Initializes condition variable COND.  A condition variable
    allows one piece of code to signal a condition and cooperating
    code to receive the signal and act upon it. */
@@ -317,8 +464,32 @@ cond_signal (struct condition *cond, struct lock *lock UNUSED)
   ASSERT (lock_held_by_current_thread (lock));
 
   if (!list_empty (&cond->waiters)) 
-    sema_up (&list_entry (list_pop_front (&cond->waiters),
-                          struct semaphore_elem, elem)->semaphore);
+  {
+	// WARNING: Possible race condition here since interrupts are on?
+	struct list_elem *e;
+	struct semaphore_elem *sem;
+	struct thread *t;
+	struct semaphore_elem *sem_to_wake = NULL;
+	int max_pri = 0;
+	for (e= list_begin (&cond->waiters); e!= list_end (&cond->waiters); e = list_next (e))
+	{
+		sem = list_entry (e, struct semaphore_elem, elem);
+		if(!list_empty(&(sem->semaphore.waiters)))
+		{	
+		
+			t= list_entry(list_front(&(sem->semaphore.waiters)),struct thread, elem);
+			if(t->priority>max_pri)
+			{
+				max_pri = t->priority;
+				sem_to_wake = sem;
+				
+			}
+		}
+		
+	}
+	list_remove(&sem_to_wake->elem);
+	sema_up(&sem_to_wake->semaphore);
+  }
 }
 
 /* Wakes up all threads, if any, waiting on COND (protected by
diff --git src/threads/synch.h src/threads/synch.h
index aab9c49..38c41e4 100644
--- src/threads/synch.h
+++ src/threads/synch.h
@@ -7,9 +7,18 @@
 /* A counting semaphore. */
 struct semaphore 
   {
-    unsigned value;             /* Current value. */
+    volatile unsigned value;             /* Current value. */
     struct list waiters;        /* List of waiting threads. */
   };
+  
+/* Struct to keep a list of threads 
+that have been put asleep*/
+struct sleeper
+{
+  int64_t sleep_time;
+  struct semaphore waiting_semaphore;
+  struct list_elem elem;
+};
 
 void sema_init (struct semaphore *, unsigned value);
 void sema_down (struct semaphore *);
@@ -17,11 +26,14 @@ bool sema_try_down (struct semaphore *);
 void sema_up (struct semaphore *);
 void sema_self_test (void);
 
+
 /* Lock. */
 struct lock 
   {
     struct thread *holder;      /* Thread holding lock (for debugging). */
     struct semaphore semaphore; /* Binary semaphore controlling access. */
+	int priority;
+	struct list_elem elem;		/* will e */
   };
 
 void lock_init (struct lock *);
@@ -29,6 +41,8 @@ void lock_acquire (struct lock *);
 bool lock_try_acquire (struct lock *);
 void lock_release (struct lock *);
 bool lock_held_by_current_thread (const struct lock *);
+void lock_donate(struct lock *);
+int lock_donate_restore(struct lock *); //return the thread priority after donation removal
 
 /* Condition variable. */
 struct condition 
diff --git src/threads/thread.c src/threads/thread.c
index d68c123..917b7bc 100644
--- src/threads/thread.c
+++ src/threads/thread.c
@@ -1,6 +1,7 @@
 #include "threads/thread.h"
 #include <debug.h>
 #include <stddef.h>
+#include "threads/malloc.h"
 #include <random.h>
 #include <stdio.h>
 #include <string.h>
@@ -11,6 +12,7 @@
 #include "threads/switch.h"
 #include "threads/synch.h"
 #include "threads/vaddr.h"
+
 #ifdef USERPROG
 #include "userprog/process.h"
 #endif
@@ -19,11 +21,15 @@
    Used to detect stack overflow.  See the big comment at the top
    of thread.h for details. */
 #define THREAD_MAGIC 0xcd6abf4b
-
 /* List of processes in THREAD_READY state, that is, processes
    that are ready to run but not actually running. */
-static struct list ready_list;
+static struct list waiting_list;
+static struct lock waiting_lock;
 
+//array of priorities containing links of threads
+//should only contain ready threads (i.e. THREAD_READY)
+static struct list priority_list [PRI_MAX+1];  
+													
 /* List of all processes.  Processes are added to this list
    when they are first scheduled and removed when they exit. */
 static struct list all_list;
@@ -51,9 +57,18 @@ static long long kernel_ticks;  /* # of timer ticks in kernel threads. */
 static long long user_ticks;    /* # of timer ticks in user programs. */
 
 /* Scheduling. */
+/* Priority scheduling */
 #define TIME_SLICE 4            /* # of timer ticks to give each thread. */
 static unsigned thread_ticks;   /* # of timer ticks since last yield. */
 
+fixed la_past_weight;
+fixed la_cur_weight;
+fixed fp_pri_max;
+
+/* BSD */
+static volatile fixed load_avg;
+static volatile fixed ready_threads;
+
 /* If false (default), use round-robin scheduler.
    If true, use multi-level feedback queue scheduler.
    Controlled by kernel command-line option "-o mlfqs". */
@@ -71,6 +86,49 @@ static void schedule (void);
 void thread_schedule_tail (struct thread *prev);
 static tid_t allocate_tid (void);
 
+inline static tid_t
+_thread_create (const char *name, int priority,
+               thread_func *function, void *aux, struct thread **t_ref);
+
+void thread_tick_ps (int64_t ticks);
+void thread_tick_mlfqs (int64_t ticks);
+void thread_set_priority_mlfqs (int new_priority);
+void thread_set_priority_ps (int new_priority);
+
+inline void thread_calc_recent_cpu (struct thread *t, void *aux UNUSED);
+inline void thread_calc_priority_mlfqs (struct thread *t, void *aux UNUSED);
+
+//#define READY_THREADS_CHECK 1
+
+#ifdef READY_THREADS_CHECK
+inline void thread_count_ready (struct thread *t, void *aux);
+#endif
+
+/* Invoke function 'func' on all threads, passing along 'aux'.
+   This function must be called with interrupts off. */
+
+// Macro version for use with an inlineable thread_action_func
+
+#define THREAD_FOREACH(FUNC,AUX) do {			\
+  struct list_elem *e;							\
+									\
+  ASSERT (intr_get_level () == INTR_OFF);				\
+									\
+  for (e = list_begin (&all_list); e != list_end (&all_list);		\
+	e = list_next (e))						\
+    {									\
+      struct thread *t = list_entry (e, struct thread, allelem);	\
+      FUNC (t, AUX);							\
+    }									\
+									\
+} while (0)
+
+void
+thread_foreach (thread_action_func *func, void *aux)
+{
+  THREAD_FOREACH(func,aux);
+}
+
 /* Initializes the threading system by transforming the code
    that's currently running into a thread.  This can't work in
    general and it is possible in this case only because loader.S
@@ -87,16 +145,48 @@ static tid_t allocate_tid (void);
 void
 thread_init (void) 
 {
+  /**
+   * Initialize appropriate scheduler
+   **/
+  if (thread_mlfqs) {
+    thread_tick = &thread_tick_mlfqs;
+    thread_set_priority = &thread_set_priority_mlfqs;
+    la_past_weight = FP_DIVI(FP_FROMINT(59),60);
+    la_cur_weight = FP_DIVI(FP_FROMINT(1),60);
+    fp_pri_max = FP_FROMINT(PRI_MAX);
+    load_avg = FP_FROMINT(0);
+    ready_threads = 0;
+    
+    printf("Past: %d Cur: %d\n",la_past_weight, la_cur_weight);
+    
+    
+  } else {
+    thread_tick = &thread_tick_ps;
+    thread_set_priority = &thread_set_priority_ps;
+  }
+  
   ASSERT (intr_get_level () == INTR_OFF);
 
   lock_init (&tid_lock);
-  list_init (&ready_list);
+  
   list_init (&all_list);
+  list_init (&waiting_list);
+  lock_init (&waiting_lock);
+
+  int i;
+  for (i = 0; i <= PRI_MAX; i++)
+    list_init(&(priority_list[i]));
 
   /* Set up a thread structure for the running thread. */
   initial_thread = running_thread ();
   init_thread (initial_thread, "main", PRI_DEFAULT);
   initial_thread->status = THREAD_RUNNING;
+  ready_threads++;
+  if (thread_mlfqs) {
+    initial_thread->nice = 0;
+    initial_thread->recent_cpu=0;
+    thread_calc_priority_mlfqs(initial_thread,NULL);
+  }
   initial_thread->tid = allocate_tid ();
 }
 
@@ -108,7 +198,8 @@ thread_start (void)
   /* Create the idle thread. */
   struct semaphore idle_started;
   sema_init (&idle_started, 0);
-  thread_create ("idle", PRI_MIN, idle, &idle_started);
+  idle_thread = NULL;
+  _thread_create ("idle", PRI_MIN, idle, &idle_started, &idle_thread);
 
   /* Start preemptive thread scheduling. */
   intr_enable ();
@@ -120,7 +211,7 @@ thread_start (void)
 /* Called by the timer interrupt handler at each timer tick.
    Thus, this function runs in an external interrupt context. */
 void
-thread_tick (void) 
+thread_tick_ps (int64_t ticks) 
 {
   struct thread *t = thread_current ();
 
@@ -134,11 +225,208 @@ thread_tick (void)
   else
     kernel_ticks++;
 
+  // wake up everything()
+  thread_wake(ticks);
   /* Enforce preemption. */
   if (++thread_ticks >= TIME_SLICE)
     intr_yield_on_return ();
 }
 
+
+#ifdef READY_THREADS_CHECK
+inline void thread_count_ready (struct thread *t, void *aux) {
+  int *count = (int *) aux;
+  
+  if ((t != idle_thread) && 
+    (t->status == THREAD_READY || t->status == THREAD_RUNNING)
+  ) {
+    (*count)++;
+  }
+}
+#endif
+
+inline void thread_calc_recent_cpu (struct thread *t, void *aux UNUSED) {
+  //PRE: has been called on a 1 second interrupt
+  fixed x = (load_avg<<1);
+  x = FP_DIV(x,FP_ADDI(x,1));
+  
+  t->recent_cpu = FP_ADDI(FP_MUL(x,t->recent_cpu),t->nice);
+}
+
+inline void thread_calc_priority_mlfqs (struct thread *t, void *aux UNUSED) {
+  // PRE: Interrupts are off
+  
+  
+#define MLFQS_CALC_PRIORITY FP_CLAMPI(FP_FLOOR( \
+    FP_SUB(fp_pri_max, \
+      FP_ADD((t->recent_cpu)>>2,FP_FROMINT(t->nice)<<1) \
+    ) \
+  ),PRI_MIN,PRI_MAX)
+  
+  int pnew = MLFQS_CALC_PRIORITY;
+  
+  if (t->status == THREAD_READY) {
+    if (pnew != t->priority) {
+      // PRE: t->elem is an element of priority_list[t->priority]
+      // TODO: Make this a debug assertion
+      list_remove(&(t->elem));
+      t->init_priority = t->priority = pnew;
+      list_push_back(&(priority_list[pnew]), &(t->elem));
+    }
+  } else {
+    t->init_priority = t->priority = pnew;
+  }
+  
+  // Floor(PRI_MAX - recent_cpu/4 - nice/2);
+}
+
+void
+thread_tick_mlfqs (int64_t ticks)
+{
+  struct thread *t = thread_current ();
+
+  /* Update statistics. */
+  
+  
+  if (t == idle_thread)
+    idle_ticks++;
+  else {
+#ifdef USERPROG
+  if (t->pagedir != NULL)
+    user_ticks++;
+  else
+    kernel_ticks++;
+#else
+    kernel_ticks++;
+#endif
+    t->recent_cpu = FP_INC(t->recent_cpu);
+  }
+  
+  if (ticks%TIMER_FREQ == 0) {
+    
+    
+    
+#ifdef READY_THREADS_CHECK    
+    // Count number of ready threads
+    int ready_threads_check = 0;
+    THREAD_FOREACH(thread_count_ready,&ready_threads_check);
+    // POST: ready_threads = number of active threads
+    if (ready_threads_check != ready_threads) {
+      printf("!!! ready_threads was %d (should be %d)\n",ready_threads,ready_threads_check);
+      ASSERT(false);
+    }
+#endif    
+    
+    // Recalculate load average
+    
+    //printf("past:%d weighted:%d by:%d  \n",load_avg, FP_MUL(la_past_weight,load_avg), la_past_weight);
+    
+    load_avg = FP_ADD(FP_MUL(la_past_weight,load_avg),FP_MULI(la_cur_weight,ready_threads));
+    
+    //printf("ready_threads:%d  load_avg:%d\n", ready_threads, thread_get_load_avg());
+    
+    // Recalculate recent_cpu
+    THREAD_FOREACH(thread_calc_recent_cpu,NULL);
+  }
+  
+  if (ticks%4 == 0) {
+    THREAD_FOREACH(thread_calc_priority_mlfqs,NULL);
+  }
+
+  // wake up everything()
+  thread_wake(ticks);
+  /* Enforce preemption. */
+  if (++thread_ticks >= TIME_SLICE)
+    intr_yield_on_return ();
+}
+
+
+
+//swap the a thread in the ready list to the
+//priority list corresponding to the priority of the
+//current thread.
+void thread_swap(struct thread *to_swap)
+{
+	enum intr_level old_level = intr_disable();
+	list_remove(&to_swap->elem);
+	to_swap->priority = thread_current()->priority;
+	list_push_back(&priority_list[thread_current()->priority],&to_swap->elem);
+	intr_set_level(old_level);
+}
+
+void thread_sleep(int64_t ticks)
+{  
+  //printf("Thread scheduled for sleep %d \n", list_size(&waiting_list));
+  
+  //lock acquired on the list
+  //put things in the list
+  //TODO: should declare them using malloc
+  
+  
+  // TODO: check for malloc() and free() errors!!
+ 
+  struct sleeper *sleeper = malloc(sizeof(struct sleeper));
+  /*if(sleeper == NULL){
+  	//out of memory check
+  }*/
+  sleeper->sleep_time = ticks;
+  sema_init(&sleeper->waiting_semaphore,0);
+  
+  
+  lock_acquire(&waiting_lock);
+  list_insert_ordered(&waiting_list, &sleeper->elem, &sleep_less, NULL);  
+  lock_release(&waiting_lock);
+
+  //put the thread to sleep  
+  sema_down(&sleeper->waiting_semaphore);
+
+  
+  lock_acquire(&waiting_lock);
+  list_remove(&sleeper->elem);
+  free(sleeper);
+  lock_release(&waiting_lock);
+}
+
+bool
+sleep_less(const struct list_elem *a, const struct list_elem *b, void *aux UNUSED)
+{
+  return list_entry(a,struct sleeper, elem)->sleep_time <  
+		list_entry(b,struct sleeper, elem) -> sleep_time;
+}
+
+inline void thread_wake(int64_t timer_ticks)
+{
+  //search through the waiting list
+  //and pick the elements that are to be woken up
+ 
+  if(!list_empty(&waiting_list)){
+    
+  
+	struct list_elem *e;
+	struct sleeper *tmp_sleeper;
+	
+	
+	//iterate throught the list of sleepers and wake up each thread
+	//that needs waking (using sema_up). Break the loop early if
+	//possible as the list is ordered in ascending order of sleep time
+	for (e= list_begin (&waiting_list); e!= list_end (&waiting_list); e = list_next (e))
+	{	
+		
+		
+		tmp_sleeper = list_entry (e, struct sleeper, elem);
+		if (tmp_sleeper->sleep_time > timer_ticks) {
+		  break;
+		}
+		else{
+			sema_up(&tmp_sleeper->waiting_semaphore); 			
+		}
+      	}
+	}
+  
+ 
+  
+}
+
 /* Prints thread statistics. */
 void
 thread_print_stats (void) 
@@ -164,9 +452,21 @@ thread_print_stats (void)
    Priority scheduling is the goal of Problem 1-3. */
 tid_t
 thread_create (const char *name, int priority,
-               thread_func *function, void *aux) 
+               thread_func *function, void *aux) {
+  return _thread_create (name, priority,
+               function, aux, NULL);
+}
+
+/**
+ * Guarantees that *t_ref will be set to the new *struct thread
+ * before the thread can be scheduled
+ **/
+inline static tid_t
+_thread_create (const char *name, int priority,
+               thread_func *function, void *aux, struct thread **t_ref) 
 {
   struct thread *t;
+  struct thread *parent;
   struct kernel_thread_frame *kf;
   struct switch_entry_frame *ef;
   struct switch_threads_frame *sf;
@@ -174,6 +474,8 @@ thread_create (const char *name, int priority,
   enum intr_level old_level;
 
   ASSERT (function != NULL);
+  
+  parent = thread_current();
 
   /* Allocate thread. */
   t = palloc_get_page (PAL_ZERO);
@@ -183,6 +485,18 @@ thread_create (const char *name, int priority,
   /* Initialize thread. */
   init_thread (t, name, priority);
   tid = t->tid = allocate_tid ();
+  
+  /**
+   * Not using a function pointer here since it's not that frequently called
+   */
+  if (thread_mlfqs) {
+    /* Inherit BSD scheduling attributes from parent */
+    t->nice = parent->nice;
+    t->recent_cpu = parent->recent_cpu;
+    
+    /* Calculate initial priority */
+    thread_calc_priority_mlfqs(t, NULL);
+  }
 
   /* Prepare thread for first run by initializing its stack.
      Do this atomically so intermediate values for the 'stack' 
@@ -205,9 +519,16 @@ thread_create (const char *name, int priority,
   sf->ebp = 0;
 
   intr_set_level (old_level);
+  
+  if (t_ref != NULL) {
+    // If we were given a reference to set
+    // this occurs before scheduling the thread
+    *t_ref = t;
+  }
 
   /* Add to run queue. */
   thread_unblock (t);
+  thread_yield();
 
   return tid;
 }
@@ -223,8 +544,17 @@ thread_block (void)
 {
   ASSERT (!intr_context ());
   ASSERT (intr_get_level () == INTR_OFF);
-
-  thread_current ()->status = THREAD_BLOCKED;
+  
+  struct thread * t = thread_current ();
+  
+  ASSERT(t->status != THREAD_DYING);
+  
+  if (t->status != THREAD_BLOCKED) {
+    t->status = THREAD_BLOCKED;
+    
+    if (t != idle_thread)
+      ready_threads--;
+  }
   schedule ();
 }
 
@@ -245,8 +575,10 @@ thread_unblock (struct thread *t)
 
   old_level = intr_disable ();
   ASSERT (t->status == THREAD_BLOCKED);
-  list_push_back (&ready_list, &t->elem);
+  list_push_back ( &( priority_list[t->priority] ), &(t->elem));
+  //list_push_back (&ready_list, &t->elem);
   t->status = THREAD_READY;
+  if (t != idle_thread) ready_threads++;
   intr_set_level (old_level);
 }
 
@@ -298,8 +630,13 @@ thread_exit (void)
      and schedule another process.  That process will destroy us
      when it calls thread_schedule_tail(). */
   intr_disable ();
-  list_remove (&thread_current()->allelem);
-  thread_current ()->status = THREAD_DYING;
+  
+  struct thread * t = thread_current();
+  
+  list_remove (&t->allelem);
+  ASSERT(t->status != THREAD_DYING);
+  t->status = THREAD_DYING;
+  ready_threads--;
   schedule ();
   NOT_REACHED ();
 }
@@ -316,34 +653,54 @@ thread_yield (void)
 
   old_level = intr_disable ();
   if (cur != idle_thread) 
-    list_push_back (&ready_list, &cur->elem);
+    list_push_back ( &( priority_list[cur->priority] ), &cur->elem);
+
   cur->status = THREAD_READY;
   schedule ();
   intr_set_level (old_level);
 }
 
-/* Invoke function 'func' on all threads, passing along 'aux'.
-   This function must be called with interrupts off. */
 void
-thread_foreach (thread_action_func *func, void *aux)
+thread_set_priority_mlfqs (int new_priority UNUSED)
 {
-  struct list_elem *e;
-
-  ASSERT (intr_get_level () == INTR_OFF);
-
-  for (e = list_begin (&all_list); e != list_end (&all_list);
-       e = list_next (e))
-    {
-      struct thread *t = list_entry (e, struct thread, allelem);
-      func (t, aux);
-    }
+    // Do nothing!
+    return;
 }
 
 /* Sets the current thread's priority to NEW_PRIORITY. */
+
+
 void
-thread_set_priority (int new_priority) 
+thread_set_priority_ps (int new_priority) 
 {
-  thread_current ()->priority = new_priority;
+  	enum intr_level old_level;
+	old_level = intr_disable();
+  	struct thread *cur_thread = thread_current();
+		cur_thread->init_priority = new_priority;
+	if(list_empty(&cur_thread->lock_list))
+	{
+	cur_thread->priority = new_priority;
+	struct list *curlist;
+	int i;
+	for(i = PRI_MAX; i>cur_thread->priority; i--)
+ 	{
+		curlist = &(priority_list[i]);
+		if(!list_empty(curlist))
+		{
+			
+				
+				break;			
+			
+		}
+	}
+	}
+	intr_set_level(old_level);
+	thread_yield();
+  //thread_current ()->priority = new_priority;
+  //need to remove it from the current priority list
+  //and put it in the new priority list
+  //need to yield the thread if there is another thread 
+  //with a higher priority (i.e. stop executing)
 }
 
 /* Returns the current thread's priority. */
@@ -357,31 +714,28 @@ thread_get_priority (void)
 void
 thread_set_nice (int nice UNUSED) 
 {
-  /* Not yet implemented. */
+  (thread_current() -> nice) = FP_CLAMPI(nice,NICE_MIN,NICE_MAX);
 }
 
 /* Returns the current thread's nice value. */
 int
 thread_get_nice (void) 
 {
-  /* Not yet implemented. */
-  return 0;
+  return (thread_current () -> nice);
 }
 
 /* Returns 100 times the system load average. */
 int
 thread_get_load_avg (void) 
 {
-  /* Not yet implemented. */
-  return 0;
+  return FP_ROUND(FP_MULI(load_avg,100));
 }
 
 /* Returns 100 times the current thread's recent_cpu value. */
 int
 thread_get_recent_cpu (void) 
 {
-  /* Not yet implemented. */
-  return 0;
+  return FP_ROUND(FP_MULI(thread_current () -> recent_cpu, 100));
 }
 
 /* Idle thread.  Executes when no other thread is ready to run.
@@ -397,7 +751,7 @@ static void
 idle (void *idle_started_ UNUSED) 
 {
   struct semaphore *idle_started = idle_started_;
-  idle_thread = thread_current ();
+  //idle_thread = thread_current ();
   sema_up (idle_started);
 
   for (;;) 
@@ -469,7 +823,16 @@ init_thread (struct thread *t, const char *name, int priority)
   t->status = THREAD_BLOCKED;
   strlcpy (t->name, name, sizeof t->name);
   t->stack = (uint8_t *) t + PGSIZE;
-  t->priority = priority;
+  
+  if (!thread_mlfqs) {
+	t->init_priority = priority;
+    t->priority = priority;
+  }
+  
+  t->try_lock = NULL;
+  //t->lock_list = malloc(sizeof(struct list));
+  //probably good to check for malloc here"
+  list_init(&t->lock_list);
   t->magic = THREAD_MAGIC;
 
   old_level = intr_disable ();
@@ -498,10 +861,29 @@ alloc_frame (struct thread *t, size_t size)
 static struct thread *
 next_thread_to_run (void) 
 {
-  if (list_empty (&ready_list))
+  static struct list * curlist;
+  //static struct list_elem *e;
+  
+  struct thread * thread_to_run = NULL;
+  int i;
+  for (i = PRI_MAX; i >=0; i--) {
+      curlist = &(priority_list[i]);
+      if(!list_empty(curlist))
+	{
+		//printf("1");
+		thread_to_run = list_entry (list_pop_front (curlist), struct thread, elem);
+		break;
+	}
+   }
+   if(thread_to_run == NULL)
+	return idle_thread;
+   else
+	return thread_to_run;
+  /*if (list_empty (&ready_list))
     return idle_thread;
   else
     return list_entry (list_pop_front (&ready_list), struct thread, elem);
+  */
 }
 
 /* Completes a thread switch by activating the new thread's page
diff --git src/threads/thread.h src/threads/thread.h
index 7965c06..b275843 100644
--- src/threads/thread.h
+++ src/threads/thread.h
@@ -3,7 +3,10 @@
 
 #include <debug.h>
 #include <list.h>
+#include "synch.h"
 #include <stdint.h>
+#include "../devices/timer.h"
+#include "fixedpoint.h"
 
 /* States in a thread's life cycle. */
 enum thread_status
@@ -24,6 +27,9 @@ typedef int tid_t;
 #define PRI_DEFAULT 31                  /* Default priority. */
 #define PRI_MAX 63                      /* Highest priority. */
 
+#define NICE_MIN -20
+#define NICE_MAX 20
+
 /* A kernel thread or user process.
 
    Each thread structure is stored in its own 4 kB page.  The
@@ -88,10 +94,19 @@ struct thread
     char name[16];                      /* Name (for debugging purposes). */
     uint8_t *stack;                     /* Saved stack pointer. */
     int priority;                       /* Priority. */
+	int init_priority;					/* Initial priority when donation starts (i.e. priority to revert to 
+										when all of the donations have been removed */
+	struct lock *try_lock;				/* Hold the lock the current thread is trying to lock on */
+	struct list lock_list;				/* Hold a list of locks that another thread is trying to acquire
+										from the current thread */
     struct list_elem allelem;           /* List element for all threads list. */
 
     /* Shared between thread.c and synch.c. */
     struct list_elem elem;              /* List element. */
+        
+    /* BSD */
+    fixed recent_cpu;
+    int nice;
 
 #ifdef USERPROG
     /* Owned by userprog/process.c. */
@@ -102,6 +117,16 @@ struct thread
     unsigned magic;                     /* Detects stack overflow. */
   };
 
+
+
+/* struct lock_donor
+{
+   int priority;  //holds the priority no.
+   struct list_elem elem;	//used in identifying priority
+}
+*/
+
+
 /* If false (default), use round-robin scheduler.
    If true, use multi-level feedback queue scheduler.
    Controlled by kernel command-line option "-o mlfqs". */
@@ -110,8 +135,13 @@ extern bool thread_mlfqs;
 void thread_init (void);
 void thread_start (void);
 
-void thread_tick (void);
+typedef void fp_thread_tick (int64_t ticks);
+fp_thread_tick *thread_tick;
+
+void thread_sleep(int64_t);
+void thread_wake(int64_t);
 void thread_print_stats (void);
+void thread_swap(struct thread *);
 
 typedef void thread_func (void *aux);
 tid_t thread_create (const char *name, int priority, thread_func *, void *);
@@ -131,11 +161,15 @@ typedef void thread_action_func (struct thread *t, void *aux);
 void thread_foreach (thread_action_func *, void *);
 
 int thread_get_priority (void);
-void thread_set_priority (int);
+
+typedef void fp_thread_set_priority (int new_priority);
+//typedef void(*fp_thread_set_priority)(int);
+fp_thread_set_priority *thread_set_priority;
 
 int thread_get_nice (void);
 void thread_set_nice (int);
 int thread_get_recent_cpu (void);
 int thread_get_load_avg (void);
+bool sleep_less(const struct list_elem *a, const struct list_elem *b, void *aux);
 
 #endif /* threads/thread.h */
